{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4167159c",
   "metadata": {},
   "source": [
    "## **Notebook to Clean and Transform applicable variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef1a1b",
   "metadata": {},
   "source": [
    "**Transformations to do:**\n",
    "\n",
    "**These columns will be excluded from the dataset:** (pdf,href formula, href, Case Number, CaseNumber.1, original order)\n",
    "\n",
    "**1. Year:** Convert to Integer, from '2025.0' to '2025'\n",
    "\n",
    "**2. Date:** convert format to real date '27th November' to '11/27/2025'. Use the Column [Year] to concatenate the year into the normalized Date\n",
    "\n",
    "**3. Type:** Normalize different iterations of the same value, example: 'Unprovoked' & 'unprovoked'\n",
    "\n",
    "**4. Country:** Only 50 nulls, explore those records to see if there is opportunity to extrapolate it from other columns like 'Location' or 'State'\n",
    "\n",
    "**5.  State:** N/A\n",
    "\n",
    "**6. Location:** N/A\n",
    "\n",
    "**7. Activity:** Normalize different iterations of the same value. Use NLP to clustter similar activities.\n",
    "\n",
    "**8. Name:** N/A\n",
    "\n",
    "**9. Sex:** Normalize different iterations of the same value\n",
    "\n",
    "**10. Age:** Remove non numerical values, explore those records\n",
    "\n",
    "**11. Injury:** Normalize different iterations of the same value and Explore creating categories (clusters?)\n",
    "\n",
    "**12. Fatal Y/N:** Normalize different iterations of the same value, this must be a binary column, maybe create a different column for values that are not Y/N or delete those records\n",
    "\n",
    "**13. Time:** Normalize to timestamp, create a new column called 'time_cluster' with 'Afternoon/Morning/Night'\n",
    "\n",
    "**14. Species:** Normalize different iterations of the same value, Create a new column with the size when available called 'shark_size' that contains meters, ie. '3m'.\n",
    "\n",
    "**15. Source:** N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d838c6d",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80306552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580afd2",
   "metadata": {},
   "source": [
    "### **Adjust Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30a3be26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working from: c:\\Users\\ynk615\\OneDrive - Corteva\n",
      "Project root: C:\\Users\\ynk615\\OneDrive - Corteva\\Documentos\\dataprojects\\sharks_p1_attacks\n",
      "Raw data: C:\\Users\\ynk615\\OneDrive - Corteva\\Documentos\\dataprojects\\sharks_p1_attacks\\data\\raw\\GSAF5.csv\n",
      "Output dir: C:\\Users\\ynk615\\OneDrive - Corteva\\Documentos\\dataprojects\\sharks_p1_attacks\\data\\processed\n",
      "\n",
      "Data shape: (7058, 21)\n"
     ]
    }
   ],
   "source": [
    "# Get the current notebook's directory\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# Navigate to project root (assuming notebook is in 'notebooks' folder)\n",
    "if NOTEBOOK_DIR.name == 'notebooks':\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    # If already in project root or elsewhere\n",
    "    PROJECT_ROOT = Path(r'C:\\Users\\ynk615\\OneDrive - Corteva\\Documentos\\dataprojects\\sharks_p1_attacks')\n",
    "\n",
    "# Define paths\n",
    "RAW_PATH = PROJECT_ROOT / 'data' / 'raw' / 'GSAF5.csv'\n",
    "OUT_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "# Create output directory\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Working from: {NOTEBOOK_DIR}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data: {RAW_PATH}\")\n",
    "print(f\"Output dir: {OUT_DIR}\")\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "print(f\"\\nData shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09196ac7",
   "metadata": {},
   "source": [
    "After data transformations\n",
    "\n",
    "- output_file = OUT_DIR / 'GSAF5_cleaned.csv'\n",
    "- df_clean.to_csv(output_file, index=False)\n",
    "- print(f\"Saved cleaned data to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59810525",
   "metadata": {},
   "source": [
    "### **Transformations & Cleanup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10d319",
   "metadata": {},
   "source": [
    "#### T&C | New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f1b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframe df to df_clean\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664728b2",
   "metadata": {},
   "source": [
    "#### T&C | Exclude Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a718fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal Y/N</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27th November</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Crowdy Bay</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Lukas Schindler</td>\n",
       "      <td>M</td>\n",
       "      <td>26</td>\n",
       "      <td>Serious leg injuries</td>\n",
       "      <td>N</td>\n",
       "      <td>0630hrs</td>\n",
       "      <td>3m Bull shark</td>\n",
       "      <td>Media: Todd Smith: Andy Currie: Simon De March...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27th November</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Crowdy Bay</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Livia Mulheim</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Y</td>\n",
       "      <td>0630hrs</td>\n",
       "      <td>3m Bull shark</td>\n",
       "      <td>Media: Todd Smith: Andy Currie: Simon De March...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10th November</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>Prevelly Beach Magaret River</td>\n",
       "      <td>Foil Boarding</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>M</td>\n",
       "      <td>61</td>\n",
       "      <td>No Injury to self</td>\n",
       "      <td>N</td>\n",
       "      <td>1745hrs</td>\n",
       "      <td>Great White Shark</td>\n",
       "      <td>Andy Currie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9th November</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>French Polynesia</td>\n",
       "      <td>Marquesas Islands</td>\n",
       "      <td>Hakahau Bay</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Not stated (Dentist)</td>\n",
       "      <td>M</td>\n",
       "      <td>40</td>\n",
       "      <td>Deep Gash to bicep</td>\n",
       "      <td>N</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>3m shark</td>\n",
       "      <td>Andrew Currie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5th November</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Pine Trees Hanalei Bay Kaui</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Chance Swanson</td>\n",
       "      <td>M</td>\n",
       "      <td>?</td>\n",
       "      <td>Injuries to legs</td>\n",
       "      <td>N</td>\n",
       "      <td>Mid afternoon</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>James Kingsley: Andy Currie: Beat of Hawaii:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date    Year        Type           Country              State  \\\n",
       "0  27th November   2025.0  Unprovoked         Australia                NSW   \n",
       "1  27th November   2025.0  Unprovoked         Australia                NSW   \n",
       "2   10th November  2025.0  Unprovoked         Australia  Western Australia   \n",
       "3    9th November  2025.0  Unprovoked  French Polynesia  Marquesas Islands   \n",
       "4    5th November  2025.0  Unprovoked               USA             Hawaii   \n",
       "\n",
       "                       Location       Activity                  Name Sex Age  \\\n",
       "0                    Crowdy Bay       Swimming       Lukas Schindler   M  26   \n",
       "1                    Crowdy Bay       Swimming         Livia Mulheim   F  25   \n",
       "2  Prevelly Beach Magaret River  Foil Boarding         Andy McDonald   M  61   \n",
       "3                   Hakahau Bay       Swimming  Not stated (Dentist)   M  40   \n",
       "4   Pine Trees Hanalei Bay Kaui       Swimming        Chance Swanson   M   ?   \n",
       "\n",
       "                 Injury Fatal Y/N           Time           Species   \\\n",
       "0  Serious leg injuries         N        0630hrs     3m Bull shark    \n",
       "1            Not stated         Y        0630hrs     3m Bull shark    \n",
       "2     No Injury to self         N        1745hrs  Great White Shark   \n",
       "3    Deep Gash to bicep         N     Not stated           3m shark   \n",
       "4      Injuries to legs         N  Mid afternoon            Unknown   \n",
       "\n",
       "                                              Source  \n",
       "0  Media: Todd Smith: Andy Currie: Simon De March...  \n",
       "1  Media: Todd Smith: Andy Currie: Simon De March...  \n",
       "2                                        Andy Currie  \n",
       "3                                      Andrew Currie  \n",
       "4       James Kingsley: Andy Currie: Beat of Hawaii:  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns that are not needed for analysis\n",
    "DROP_COLS = [\"pdf\", \"href formula\", \"href\",\"Case Number\", \"Case Number.1\", \"original order\"]\n",
    "\n",
    "df_clean = df_clean.drop(columns=[c for c in DROP_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "df_clean.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228feef",
   "metadata": {},
   "source": [
    "#### 1. T&C | Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "857ffd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal Y/N</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27th November</td>\n",
       "      <td>2025</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Crowdy Bay</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Lukas Schindler</td>\n",
       "      <td>M</td>\n",
       "      <td>26</td>\n",
       "      <td>Serious leg injuries</td>\n",
       "      <td>N</td>\n",
       "      <td>0630hrs</td>\n",
       "      <td>3m Bull shark</td>\n",
       "      <td>Media: Todd Smith: Andy Currie: Simon De March...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27th November</td>\n",
       "      <td>2025</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Crowdy Bay</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Livia Mulheim</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>Y</td>\n",
       "      <td>0630hrs</td>\n",
       "      <td>3m Bull shark</td>\n",
       "      <td>Media: Todd Smith: Andy Currie: Simon De March...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10th November</td>\n",
       "      <td>2025</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>Prevelly Beach Magaret River</td>\n",
       "      <td>Foil Boarding</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>M</td>\n",
       "      <td>61</td>\n",
       "      <td>No Injury to self</td>\n",
       "      <td>N</td>\n",
       "      <td>1745hrs</td>\n",
       "      <td>Great White Shark</td>\n",
       "      <td>Andy Currie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9th November</td>\n",
       "      <td>2025</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>French Polynesia</td>\n",
       "      <td>Marquesas Islands</td>\n",
       "      <td>Hakahau Bay</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Not stated (Dentist)</td>\n",
       "      <td>M</td>\n",
       "      <td>40</td>\n",
       "      <td>Deep Gash to bicep</td>\n",
       "      <td>N</td>\n",
       "      <td>Not stated</td>\n",
       "      <td>3m shark</td>\n",
       "      <td>Andrew Currie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5th November</td>\n",
       "      <td>2025</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Pine Trees Hanalei Bay Kaui</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Chance Swanson</td>\n",
       "      <td>M</td>\n",
       "      <td>?</td>\n",
       "      <td>Injuries to legs</td>\n",
       "      <td>N</td>\n",
       "      <td>Mid afternoon</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>James Kingsley: Andy Currie: Beat of Hawaii:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  Year        Type           Country              State  \\\n",
       "0  27th November   2025  Unprovoked         Australia                NSW   \n",
       "1  27th November   2025  Unprovoked         Australia                NSW   \n",
       "2   10th November  2025  Unprovoked         Australia  Western Australia   \n",
       "3    9th November  2025  Unprovoked  French Polynesia  Marquesas Islands   \n",
       "4    5th November  2025  Unprovoked               USA             Hawaii   \n",
       "\n",
       "                       Location       Activity                  Name Sex Age  \\\n",
       "0                    Crowdy Bay       Swimming       Lukas Schindler   M  26   \n",
       "1                    Crowdy Bay       Swimming         Livia Mulheim   F  25   \n",
       "2  Prevelly Beach Magaret River  Foil Boarding         Andy McDonald   M  61   \n",
       "3                   Hakahau Bay       Swimming  Not stated (Dentist)   M  40   \n",
       "4   Pine Trees Hanalei Bay Kaui       Swimming        Chance Swanson   M   ?   \n",
       "\n",
       "                 Injury Fatal Y/N           Time           Species   \\\n",
       "0  Serious leg injuries         N        0630hrs     3m Bull shark    \n",
       "1            Not stated         Y        0630hrs     3m Bull shark    \n",
       "2     No Injury to self         N        1745hrs  Great White Shark   \n",
       "3    Deep Gash to bicep         N     Not stated           3m shark   \n",
       "4      Injuries to legs         N  Mid afternoon            Unknown   \n",
       "\n",
       "                                              Source  \n",
       "0  Media: Todd Smith: Andy Currie: Simon De March...  \n",
       "1  Media: Todd Smith: Andy Currie: Simon De March...  \n",
       "2                                        Andy Currie  \n",
       "3                                      Andrew Currie  \n",
       "4       James Kingsley: Andy Currie: Beat of Hawaii:  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to Integer, from '2025.0' to '2025'\n",
    "df_clean['Year'] = pd.to_numeric(df_clean['Year'], errors='coerce').astype('Int64')\n",
    "\n",
    "df_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81356c",
   "metadata": {},
   "source": [
    "#### 2. T&C | Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6b843",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Messy dataset, lots of different formats and some entries with no dates at all (ie. 'No date', 'Early 1930s','World War II', etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2901e553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed: 6421 out of 7058 records\n",
      "Failed to parse: 637 records\n",
      "Success rate: 91.0%\n",
      "\n",
      "Records that could not be parsed:\n",
      "              Date_Original  Year\n",
      "178   Reported 14-June 2023  2023\n",
      "220            29 Jan--2023  2023\n",
      "221            08-Jan--2023  2023\n",
      "224                  23-Jan  2023\n",
      "326            11-Dec-2021`  2021\n",
      "493              10-Jul-202  2020\n",
      "713   Reported 09-Jul-2018.  2018\n",
      "724                Jun-1018  2018\n",
      "732                  18-May  2018\n",
      "808                  17-Sep  2017\n",
      "853              2017.06.05  2017\n",
      "905             00.Feb.2017  2017\n",
      "912    Reported 08-Jan-2017  <NA>\n",
      "953                  16-Sep  2016\n",
      "1008                 16-May  2016\n",
      "1098                 15-Sep  2015\n",
      "1231                 14-Sep  2014\n",
      "1252                 14-Aug  2014\n",
      "1446                 12-Dec  2012\n",
      "1529                 13-Apr  2012\n"
     ]
    }
   ],
   "source": [
    "def parse_date_flexible(row):\n",
    "    \"\"\"\n",
    "    Try to parse various date formats. Return NaT if unable to parse.\n",
    "    \"\"\"\n",
    "    date_str = str(row['Date']).strip()\n",
    "    year = row['Year']\n",
    "    \n",
    "    # Handle completely non-date values\n",
    "    non_dates = ['no date', 'before', 'after', 'circa', 'early', 'late', 'mid', \n",
    "                 'summer', 'winter', 'fall', 'spring', 'world war', 'ca .', \n",
    "                 'between', 'prior', 'letter', 'last incident', 'probably']\n",
    "    \n",
    "    if any(nd in date_str.lower() for nd in non_dates):\n",
    "        return pd.NaT\n",
    "    \n",
    "    # Handle pure numbers (Excel date serial numbers)\n",
    "    if date_str.isdigit():\n",
    "        try:\n",
    "            # Convert Excel serial date to datetime\n",
    "            date_obj = pd.to_datetime('1899-12-30') + pd.Timedelta(days=int(date_str))\n",
    "            return date_obj\n",
    "        except:\n",
    "            return pd.NaT\n",
    "    \n",
    "    # Remove \"Reported\" prefix\n",
    "    date_str = pd.Series(date_str).str.replace(r'Reported\\s+', '', regex=True, case=False).iloc[0]\n",
    "    \n",
    "    # Remove ordinal suffixes (st, nd, rd, th)\n",
    "    date_clean = pd.Series(date_str).str.replace(r'(\\d+)(st|nd|rd|th)', r'\\1', regex=True).iloc[0]\n",
    "    \n",
    "    # Try different date formats\n",
    "    formats_to_try = [\n",
    "        ('%d %B', True),           # 10 November (needs year appended)\n",
    "        ('%d %B %Y', False),       # 10 November 2025 (has year)\n",
    "        ('%d-%b-%Y', False),       # 03-Dec-1882\n",
    "        ('%d-%b-%y', True),        # 11-Jun-25 (needs year replacement)\n",
    "        ('%b-%Y', False),          # Apr-1863\n",
    "        ('%b-%d-%Y', False),       # Aug-24-1806\n",
    "        ('%d %b-%Y', False),       # 02 Nov-2023\n",
    "    ]\n",
    "    \n",
    "    # Try parsing with each format\n",
    "    for fmt, needs_year_append in formats_to_try:\n",
    "        try:\n",
    "            if needs_year_append:\n",
    "                # For formats like \"%d %B\" that don't include year\n",
    "                date_obj = pd.to_datetime(f\"{date_clean} {year}\", format=f\"{fmt} %Y\")\n",
    "            else:\n",
    "                # For formats that already have year\n",
    "                date_obj = pd.to_datetime(date_clean, format=fmt)\n",
    "                # Replace with Year column value if available\n",
    "                if year and not pd.isna(year):\n",
    "                    date_obj = date_obj.replace(year=int(year))\n",
    "            return date_obj\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # If all formats fail, return NaT\n",
    "    return pd.NaT\n",
    "\n",
    "# Apply the parsing function\n",
    "df_clean['Date_Original'] = df_clean['Date']  # Keep original for reference\n",
    "df_clean['Date_Parsed'] = df_clean.apply(parse_date_flexible, axis=1)\n",
    "\n",
    "# Create a flag for successfully parsed dates\n",
    "df_clean['Date_Valid'] = ~df_clean['Date_Parsed'].isna()\n",
    "\n",
    "# Format valid dates as MM/DD/YYYY\n",
    "df_clean['Date'] = df_clean['Date_Parsed'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "# Check how many dates were successfully parsed\n",
    "print(f\"Successfully parsed: {df_clean['Date_Valid'].sum()} out of {len(df_clean)} records\")\n",
    "print(f\"Failed to parse: {(~df_clean['Date_Valid']).sum()} records\")\n",
    "print(f\"Success rate: {df_clean['Date_Valid'].sum() / len(df_clean) * 100:.1f}%\")\n",
    "\n",
    "# View records that failed to parse\n",
    "if (~df_clean['Date_Valid']).sum() > 0:\n",
    "    print(\"\\nRecords that could not be parsed:\")\n",
    "    print(df_clean[~df_clean['Date_Valid']][['Date_Original', 'Year']].head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
